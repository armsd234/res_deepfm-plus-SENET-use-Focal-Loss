{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23bafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler,StandardScaler\n",
    "\n",
    "\n",
    "class SENET(layers.Layer):\n",
    "    def __init__(self, field_size,emb_size, r):\n",
    "        super(SENET, self).__init__()\n",
    "        self.f = field_size\n",
    "        self.m = emb_size\n",
    "        self.MLP1 = layers.Dense(units=(field_size//r), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.MLP2 = layers.Dense(units=field_size,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs,shape=[-1, self.f, self.m])\n",
    "        x = inputs\n",
    "        x = tf.reduce_mean(x, axis=2)\n",
    "        x = tf.reshape(x,shape=[-1, self.f])\n",
    "        x = self.MLP1(x)\n",
    "        x = self.MLP2(x)\n",
    "        outputs = inputs*tf.reshape(x,shape=[-1, self.f,1])\n",
    "        return tf.reshape(outputs,shape=[-1, self.f*self.m])\n",
    "\n",
    "\n",
    "class ResNet(layers.Layer):\n",
    "    def __init__(self, hidden_unit, dim_stack):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = layers.Dense(units=hidden_unit, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.layer2 = layers.Dense(units=dim_stack,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        outputs = x + inputs\n",
    "        return outputs\n",
    "\n",
    "class DeepFM(Model):\n",
    "    def __init__(self, spare_feature_columns, dense_feature_columns, k, w_reg, v_reg, hidden_units, output_dim, activation, drop_out,Use_DNN=True,Use_Res=False):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.spare_feature_columns = spare_feature_columns\n",
    "        self.dense_feature_columns = dense_feature_columns\n",
    "        self.w_reg = w_reg\n",
    "        self.v_reg = v_reg\n",
    "        self.k = k\n",
    "        self.Use_DNN = Use_DNN\n",
    "        self.Use_Res = Use_Res\n",
    "\n",
    "        # embedding\n",
    "        self.embedding_layer = {'embed_layer{}'.format(i): layers.Embedding(feat['vocabulary_size'], self.k)\n",
    "                                for i, feat in enumerate(self.spare_feature_columns)}\n",
    "\n",
    "        # 做完embedding后的维度\n",
    "        self.dense_dim = len(self.dense_feature_columns)\n",
    "        self.spare_dim = len(self.spare_feature_columns)*self.k\n",
    "        self.onedim = self.dense_dim + self.spare_dim \n",
    "        \n",
    "        self.SENET = tf.keras.Sequential()\n",
    "        self.SENET.add(SENET(len(self.spare_feature_columns), self.k, 2))\n",
    "        self.deep_dim = self.onedim + self.spare_dim\n",
    "\n",
    "        if(self.Use_Res):\n",
    "            #Res\n",
    "            self.DNN = tf.keras.Sequential()\n",
    "            for hidden in hidden_units:\n",
    "                self.DNN.add(ResNet(hidden,self.deep_dim))\n",
    "                self.DNN.add(layers.BatchNormalization())\n",
    "                self.DNN.add(layers.Activation(activation))\n",
    "                self.DNN.add(layers.Dropout(drop_out))\n",
    "            self.DNN.add(layers.Dense(output_dim, activation=None))    \n",
    "        \n",
    "        if(self.Use_DNN):\n",
    "            # dnn\n",
    "            self.DNN = tf.keras.Sequential()\n",
    "            for hidden in hidden_units:\n",
    "                self.DNN.add(layers.Dense(hidden, kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "                self.DNN.add(layers.BatchNormalization())\n",
    "                self.DNN.add(layers.Activation(activation))\n",
    "                self.DNN.add(layers.Dropout(drop_out))\n",
    "            self.DNN.add(layers.Dense(output_dim, activation=None))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(name='b', shape=(1,), initializer=tf.zeros_initializer(), trainable=True, )\n",
    "        self.w = self.add_weight(name='w', shape=(self.onedim, 1), initializer=tf.random_normal_initializer(), trainable=True, regularizer=tf.keras.regularizers.l2(self.w_reg))\n",
    "        self.v = self.add_weight(name='v', shape=(self.onedim, self.k), initializer=tf.random_normal_initializer(), trainable=True, regularizer=tf.keras.regularizers.l2(self.v_reg))\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # dense_inputs: 数值特征，13维\n",
    "        # sparse_inputs： 类别特征，26维\n",
    "        dense_inputs, sparse_inputs = inputs[:, :13], inputs[:, 13:]\n",
    "\n",
    "        # embedding\n",
    "        sparse_embed = tf.concat([self.embedding_layer['embed_layer{}'.format(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])], axis=1)  # (batchsize, 26*k)\n",
    "        \n",
    "        SENET_embed = self.SENET(sparse_embed)\n",
    "        \n",
    "        # FM、Deep 共享embedding\n",
    "        FM_x = tf.concat([dense_inputs, sparse_embed], axis=1)  # (batchsize, 26*embed_dim + 13)\n",
    "       \n",
    "        deep_x = tf.concat([FM_x, SENET_embed], axis=1)\n",
    "       \n",
    "\n",
    "        # FM part\n",
    "        linear_part = tf.matmul(FM_x, self.w) + self.b  # (batchsize, 1)\n",
    "        inter_cross1 = tf.square(FM_x @ self.v)  # (batchsize, k)\n",
    "        inter_cross2 = tf.matmul(tf.pow(FM_x, 2), tf.pow(self.v, 2))  # (batchsize, k)\n",
    "        cross_part = 0.5 * tf.reduce_sum(inter_cross1 - inter_cross2, axis=1, keepdims=True)  # (batchsize, 1)\n",
    "        fm_output = linear_part + cross_part\n",
    "\n",
    "        # Deep part\n",
    "        dnn_out = self.DNN(deep_x)  # (batchsize, 1)\n",
    "        \n",
    "        output = tf.nn.sigmoid(fm_output + dnn_out)\n",
    "        #output = tf.nn.sigmoid(fm_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d67ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseFeature(feat, vocabulary_size, embed_dim):\n",
    "    return {'feat': feat, 'vocabulary_size': vocabulary_size, 'embed_dim': embed_dim}\n",
    "\n",
    "def denseFeature(feat):\n",
    "    return {'feat': feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd1d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def Focal_Loss(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    loss = 0.8*K.pow(1.0 - y_pred,1.0)*y_true * K.log(y_pred + 1e-10)+0.2*K.pow(y_pred,1.0)*(1.0 - y_true) * K.log(1.0 - y_pred + 1e-10)\n",
    "    #loss = 0.5*y_true * K.log(y_pred + 1e-15)+0.5*(1.0 - y_true) * K.log(1.0 - y_pred + 1e-15)\n",
    "    #loss = y_true * K.log(y_pred + 1e-10)+(1.0 - y_true) * K.log(1.0 - y_pred + 1e-10)\n",
    "    return -K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d92dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # I1-I13：总共 13 列数值型特征\n",
    "    # C1-C26：共有 26 列类别型特征\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    target_columns = ['label']\n",
    "    columns = target_columns + dense_features + sparse_features\n",
    "    data = pd.read_csv(\"./data/dac/train.txt\",sep='\\t',names = columns, nrows=200000)\n",
    "    \n",
    "    data[dense_features] = data[dense_features].fillna(0.0)\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1')\n",
    "    \n",
    "    #data[dense_features] = StandardScaler().fit_transform(data[dense_features])\n",
    "        \n",
    "    for f in dense_features:\n",
    "        data[f] = data[f].apply(lambda x: np.log(x+1) if x>-1 else -1)\n",
    "    data[dense_features] = MinMaxScaler().fit_transform(data[dense_features])\n",
    "\n",
    "    for f in sparse_features:\n",
    "        data[f] = LabelEncoder().fit_transform(data[f])\n",
    "    \n",
    "    data_X = data.iloc[:, 1:]\n",
    "    data_y = data['label'].values\n",
    "\n",
    "    dense_feature_columns = [denseFeature(feat) for feat in dense_features]\n",
    "    spare_feature_columns = [sparseFeature(feat, data_X[feat].nunique(),3) for feat in sparse_features]\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.2, random_state=0, stratify=data_y)\n",
    "    \n",
    "    #tmp_X, test_X, tmp_y, test_y = train_test_split(data_X, data_y, test_size=0.2, random_state=42, stratify=data_y)\n",
    "    #train_X, val_X, train_y, val_y = train_test_split(tmp_X, tmp_y, test_size=0.2, random_state=42, stratify=tmp_y)\n",
    "\n",
    "#     model = DeepFM(spare_feature_columns = spare_feature_columns,\n",
    "#                    dense_feature_columns = dense_feature_columns,\n",
    "#                    k = 15,\n",
    "#                    w_reg = 0.01,\n",
    "#                    v_reg = 0.001,\n",
    "#                    hidden_units= [50,50,50,50,50],\n",
    "#                    output_dim = 1,\n",
    "#                    activation = 'relu',\n",
    "#                    drop_out = 0.7,\n",
    "#                    Use_DNN=False,\n",
    "#                    Use_Res=True)\n",
    "\n",
    "#     adam = optimizers.Adam(lr=0.005, decay=0.001)\n",
    "     \n",
    "#     model.compile(\n",
    "#         optimizer=adam,\n",
    "#         loss=Focal_Loss,\n",
    "#         metrics=[metrics.AUC(), metrics.Recall()]\n",
    "#     )\n",
    "    \n",
    "#     log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#     model.fit(\n",
    "#         train_X.values, train_y,\n",
    "#         validation_data=(test_X.values, test_y),\n",
    "#         batch_size=2048,\n",
    "#         epochs=5,\n",
    "#         verbose=1,\n",
    "#         callbacks=[tensorboard_callback]\n",
    "#     )\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88497cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trian(emb_k,units,dropout,spare_column,dense_column,train_X,train_y,test_X, test_y):\n",
    "    model = DeepFM(spare_feature_columns = spare_column,\n",
    "                   dense_feature_columns = dense_column,\n",
    "                   k = emb_k,\n",
    "                   w_reg = 0.01,\n",
    "                   v_reg = 0.001,\n",
    "                   hidden_units= units,\n",
    "                   output_dim = 1,\n",
    "                   activation = 'relu',\n",
    "                   drop_out = dropout,\n",
    "                   Use_DNN=False,\n",
    "                   Use_Res=True)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.005, decay=0.001)\n",
    "     \n",
    "    model.compile(\n",
    "        optimizer=adam,\n",
    "        loss = Focal_Loss,\n",
    "        #loss='binary_crossentropy',\n",
    "        metrics=[metrics.AUC()]\n",
    "    )\n",
    "    units = [str(x) for x in units]\n",
    "    log_dir=\"logs/last/%s_%s_layer%s\" %(emb_k,dropout,\"_\".join(units))  \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#     if emb_k==5 or emb_k==10:\n",
    "#         epoch=8\n",
    "#     else:\n",
    "#         epoch=5 loss=\"binary_crossentropy\"\n",
    "        \n",
    "    model.fit(\n",
    "        train_X.values, train_y,\n",
    "        validation_data=(test_X.values, test_y),\n",
    "        batch_size=2048,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3fc621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/79 [..............................] - ETA: 0s - loss: 38.9127 - auc: 0.4979WARNING:tensorflow:From F:\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/79 [..............................] - ETA: 40s - loss: 32.2868 - auc: 0.6107WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4004s vs `on_train_batch_end` time: 0.6571s). Check your callbacks.\n",
      "79/79 [==============================] - 27s 339ms/step - loss: 2.1186 - auc: 0.7126 - val_loss: 0.1272 - val_auc: 0.7674\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 25s 310ms/step - loss: 0.0954 - auc: 0.8329 - val_loss: 0.1249 - val_auc: 0.7738\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 25s 312ms/step - loss: 0.0740 - auc: 0.9144 - val_loss: 0.1408 - val_auc: 0.7568\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 25s 317ms/step - loss: 0.0657 - auc: 0.9409 - val_loss: 0.1373 - val_auc: 0.7361\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 25s 315ms/step - loss: 0.0637 - auc: 0.9512 - val_loss: 0.1530 - val_auc: 0.7218\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 24s 305ms/step - loss: 0.0584 - auc: 0.9567 - val_loss: 0.1424 - val_auc: 0.7158\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 25s 313ms/step - loss: 0.0574 - auc: 0.9598 - val_loss: 0.1492 - val_auc: 0.7272\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 26s 330ms/step - loss: 0.0553 - auc: 0.9618 - val_loss: 0.1878 - val_auc: 0.7221\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 25s 319ms/step - loss: 0.0525 - auc: 0.9642 - val_loss: 0.1959 - val_auc: 0.7101\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 26s 330ms/step - loss: 0.0527 - auc: 0.9652 - val_loss: 0.1865 - val_auc: 0.7187\n"
     ]
    }
   ],
   "source": [
    "for units in [[4000]]:\n",
    "    for k in [24]:\n",
    "        for dropout in [0.7]:\n",
    "            trian(k,units,dropout,spare_feature_columns,dense_feature_columns,train_X,train_y,test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582f5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62feafa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e687f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10ddfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7296f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0e39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c664c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6af6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5090f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fae744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91052262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
